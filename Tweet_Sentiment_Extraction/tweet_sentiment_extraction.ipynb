{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_sentiment_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWtsTwXCOw7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cf7c934a-7cef-4bdf-c48f-6c9c190e8d76"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erbMa4__79Bf",
        "colab_type": "text"
      },
      "source": [
        "#### Imports and TPU setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8LxAXfCwdYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "078d6380-4e3d-488c-adac-c3c1451fedf4"
      },
      "source": [
        "! pip uninstall kaggle -q\n",
        "! pip install kaggle -q\n",
        "! pip install transformers -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proceed (y/n)? y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQDCshNp2v52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9dc5d35b-dc27-4ca1-a044-7fbbe2839f7a"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import tokenizers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.keras.backend as K\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFRobertaModel\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSm866_x7d1s",
        "colab_type": "text"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLGJ4FpRwn8e",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "22f09f26-dc37-4520-970a-ff4e0241147a"
      },
      "source": [
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c 'tweet-sentiment-extraction'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c2f687ff-ab1c-4f74-996a-8ebbaeb1912f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c2f687ff-ab1c-4f74-996a-8ebbaeb1912f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading tweet-sentiment-extraction.zip to /content\n",
            "  0% 0.00/1.39M [00:00<?, ?B/s]\n",
            "100% 1.39M/1.39M [00:00<00:00, 84.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMpit1gy1hBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "97deae6a-ba59-4729-988a-ff3c13367f3a"
      },
      "source": [
        "!unzip '/content/tweet-sentiment-extraction.zip'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/tweet-sentiment-extraction.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qvnS8kP1OpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "train['text'] = train['text'].astype(str)\n",
        "train['selected_text'] = train['selected_text'].astype(str)\n",
        "\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "test['text'] = test['text'].astype(str)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAkSXTrLrVVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d415b198-209c-410a-8e55-c9472bef7054"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "0  cb774db0d1  ...   neutral\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuzsY5Q33UhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from text import  *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_MXXvDw46Va",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "815635dd-b22f-47ac-8f39-797d73687b45"
      },
      "source": [
        "train.text.values[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' I`d have responded, if I were going',\n",
              "       ' Sooo SAD I will miss you here in San Diego!!!',\n",
              "       'my boss is bullying me...', ' what interview! leave me alone'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L40QcD0o3X_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d91636fe-81f1-4fc4-a260-a23fa98e3551"
      },
      "source": [
        "get_translation(get_translation(list(train.text.values[:4]), dest_lang='de'), dest_lang='en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I would have answered if I left',\n",
              " 'Sooo sad, I will miss you here in San Diego !!!',\n",
              " 'My boss is harassing me ...',\n",
              " 'what an interview! leave me alone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joA3n5aK8CZL",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "130psWX0F5kS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "983335cd-c2ca-423f-eb3e-53e69e318f0a"
      },
      "source": [
        "print(f\"train shape: {train.shape} \\ntest shape: {test.shape}\")\n",
        "print(f\"ratio between lables in test: {test.sentiment.value_counts()[1] / test.sentiment.value_counts()[0]}\")\n",
        "print(\"-\"*4)\n",
        "mean_word_len = train.text.apply(lambda x: len(x.split(\" \"))).mean()\n",
        "print(f\"Dataset with shape of {train.shape[0]} samples. \\nMean number of words is: {mean_word_len}. \\nDistribution of lables is: \\n{train.sentiment.value_counts()}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape: (27481, 4) \n",
            "test shape: (3534, 3)\n",
            "ratio between lables in test: 0.7713286713286713\n",
            "----\n",
            "Dataset with shape of 27481 samples. \n",
            "Mean number of words is: 13.7794476183545. \n",
            "Distribution of lables is: \n",
            "neutral     11118\n",
            "positive     8582\n",
            "negative     7781\n",
            "Name: sentiment, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB3CzUAlCRnq",
        "colab_type": "text"
      },
      "source": [
        "#### Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6dgn0mZRC8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 96"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3Ckz5iPxZDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", lowercase = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoDJ1-QZX3OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = \"/content/drive/My Drive/projects/tweet sentiment extraction\"\n",
        "\n",
        "# tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "#     vocab_file=path + os.sep + 'vocab.json', \n",
        "#     merges_file=path + os.sep + 'merges.txt', \n",
        "#     lowercase=True,\n",
        "#     add_prefix_space=True\n",
        "# )"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwq0v6WReCl0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82938693-42ac-405c-c5fe-52a7a946a74b"
      },
      "source": [
        "sentiment_id = {'positive': tokenizer.encode_plus('positive', add_prefix_space=True, max_length = MAX_LEN, pad_to_max_length = True, return_attention_mask = True, return_token_type_ids = True, truncation=True).input_ids[1], \n",
        "                'negative': tokenizer.encode_plus('negative', add_prefix_space=True, max_length = MAX_LEN, pad_to_max_length = True, return_attention_mask = True, return_token_type_ids = True, truncation=True).input_ids[1], \n",
        "                'neutral':  tokenizer.encode_plus('neutral', add_prefix_space=True, max_length = MAX_LEN, pad_to_max_length = True, return_attention_mask = True, return_token_type_ids = True, truncation=True).input_ids[1]}\n",
        "sentiment_id"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'negative': 2430, 'neutral': 7974, 'positive': 1313}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dinbgh-0IJK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "634f0a76-77f6-4e66-8c35-b7a5d616f370"
      },
      "source": [
        "for i in range(4):\n",
        "  print(f\"Special token {i} -> {tokenizer.decode([i])}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special token 0 -> <s>\n",
            "Special token 1 -> <pad>\n",
            "Special token 2 -> </s>\n",
            "Special token 3 -> <unk>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InGBcEjsttIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "# def jaccard(str1, str2): \n",
        "#     a = set(str1.lower().split()) \n",
        "#     b = set(str2.lower().split())\n",
        "#     if (len(a)==0) & (len(b)==0): return 0.5\n",
        "#     c = a.intersection(b)\n",
        "#     return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNQCIICekG5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b67b347-f249-476c-fe88-d74e0734cdce"
      },
      "source": [
        "ct = train.shape[0]\n",
        "\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in tqdm(range(train.shape[0])):\n",
        "    text1 = \" \" + \" \".join(train.loc[k,'text'].split())\n",
        "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
        "\n",
        "    enc = tokenizer.encode_plus(text1.lower(), \n",
        "                       add_prefix_space=True,\n",
        "                       max_length = MAX_LEN, # max length of the text that can go to BERT\n",
        "                       pad_to_max_length = True, # add [PAD] tokens\n",
        "                       return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "                       return_token_type_ids = True,\n",
        "                       truncation=True)\n",
        "    \n",
        "    input_ids[k] = np.array(enc.input_ids)\n",
        "    input_ids[k, np.where(input_ids[k] == 2)[0][0]+1 : np.where(input_ids[k] == 2)[0][0]+4] = 2\n",
        "    input_ids[k, np.where(input_ids[k] == 2)[0][0]+2] = sentiment_id[train.loc[k,'sentiment']]\n",
        "\n",
        "    attention_mask[k] = np.array(enc.attention_mask)\n",
        "    attention_mask[k, np.where(attention_mask[k] == 1)[0][-1] + 1 : np.where(attention_mask[k] == 1)[0][-1] + 4] = 1\n",
        "   \n",
        "    token_type_ids[k] = np.array(enc.token_type_ids)\n",
        "\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)] = 1\n",
        "\n",
        "    if text1[idx-1] == ' ': \n",
        "        chars[idx-1] = 1 \n",
        "        \n",
        "    idx=0\n",
        "    toks = []\n",
        "    for i, t in enumerate(enc.input_ids[1:]):\n",
        "          w = tokenizer.decode([t])\n",
        "          if np.sum(chars[idx:idx+len(w)]) > 0: # if we are in overlapp location append the token\n",
        "              toks.append(i)\n",
        "          idx += len(w)\n",
        "    \n",
        "    if len(toks) > 0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 27481/27481 [00:57<00:00, 480.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo8SaG8sNZlY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cadaa658-be68-413c-9b77-cf446a8a5075"
      },
      "source": [
        "ct = test.shape[0]\n",
        "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in tqdm(range(test.shape[0])):\n",
        "        \n",
        "    text1 = \" \" + \" \".join(test.loc[k,'text'].split())\n",
        "\n",
        "    enc = tokenizer.encode_plus(text1.lower(), \n",
        "                       add_prefix_space=True,\n",
        "                       max_length = MAX_LEN, # max length of the text that can go to BERT\n",
        "                       pad_to_max_length = True, # add [PAD] tokens\n",
        "                       return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "                       return_token_type_ids = True,\n",
        "                       truncation=True)\n",
        "    \n",
        "    input_ids_t[k] = np.array(enc.input_ids)\n",
        "    input_ids_t[k, np.where(input_ids_t[k] == 2)[0][0]+1 : np.where(input_ids_t[k] == 2)[0][0]+4] = 2\n",
        "    input_ids_t[k, np.where(input_ids_t[k] == 2)[0][0]+2] = sentiment_id[test.loc[k,'sentiment']]\n",
        "\n",
        "    attention_mask_t[k] = np.array(enc.attention_mask)\n",
        "    attention_mask_t[k, np.where(attention_mask_t[k] == 1)[0][-1] + 1 : np.where(attention_mask_t[k] == 1)[0][-1] + 4] = 1\n",
        "   \n",
        "    token_type_ids_t[k] = np.array(enc.token_type_ids)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3534/3534 [00:01<00:00, 1855.13it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otT7hI3B8E9G",
        "colab_type": "text"
      },
      "source": [
        "###### Build model inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ApG9lKa8IJF",
        "colab_type": "text"
      },
      "source": [
        "###### Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOEhMdKDNy75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "    x = roberta_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY7_dizOJ2_j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "73f37701-8b5b-48c1-f586-653dc4b5e048"
      },
      "source": [
        "K.clear_session()\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 96)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 96, 768), (N 124645632   input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 96, 768)      0           tf_roberta_model[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 96, 768)      0           tf_roberta_model[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 96, 1)        769         dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 96, 1)        769         dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 96)           0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 96)           0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 96)           0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 96)           0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 124,647,170\n",
            "Trainable params: 124,647,170\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg1lCLqO8DiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "965cb4e1-376e-4ecd-8628-945b14e9fe82"
      },
      "source": [
        "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print(f\"### FOLD {fold+1}\")\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "        \n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
        "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
        "    \n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('#'*5)\n",
        "    print(\"Predicting Validation\")\n",
        "    print('#'*5)\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print('#'*5)\n",
        "    print(\"Predicting Test\")\n",
        "    print('#'*5)\n",
        "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:            \n",
        "            st = tokenizer.decode(input_ids[k][a:b+1])\n",
        "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print(f\"FOLD {fold+1} Jaccard {np.mean(all)}\")\n",
        "    print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "687/687 [==============================] - ETA: 0s - loss: 2.3697 - activation_loss: 1.1048 - activation_1_loss: 1.2649\n",
            "Epoch 00001: val_loss improved from inf to 1.83191, saving model to v0-roberta-0.h5\n",
            "687/687 [==============================] - 278s 405ms/step - loss: 2.3697 - activation_loss: 1.1048 - activation_1_loss: 1.2649 - val_loss: 1.8319 - val_activation_loss: 0.8736 - val_activation_1_loss: 0.9583\n",
            "Epoch 2/3\n",
            "687/687 [==============================] - ETA: 0s - loss: 1.7494 - activation_loss: 0.8594 - activation_1_loss: 0.8900\n",
            "Epoch 00002: val_loss improved from 1.83191 to 1.68792, saving model to v0-roberta-0.h5\n",
            "687/687 [==============================] - 278s 405ms/step - loss: 1.7494 - activation_loss: 0.8594 - activation_1_loss: 0.8900 - val_loss: 1.6879 - val_activation_loss: 0.8518 - val_activation_1_loss: 0.8361\n",
            "Epoch 3/3\n",
            "687/687 [==============================] - ETA: 0s - loss: 1.5501 - activation_loss: 0.7905 - activation_1_loss: 0.7596\n",
            "Epoch 00003: val_loss improved from 1.68792 to 1.67941, saving model to v0-roberta-0.h5\n",
            "687/687 [==============================] - 278s 405ms/step - loss: 1.5501 - activation_loss: 0.7905 - activation_1_loss: 0.7596 - val_loss: 1.6794 - val_activation_loss: 0.8508 - val_activation_1_loss: 0.8286\n",
            "#####\n",
            "Predicting Validation\n",
            "#####\n",
            "172/172 [==============================] - 25s 145ms/step\n",
            "#####\n",
            "Predicting Test\n",
            "#####\n",
            "111/111 [==============================] - 16s 145ms/step\n",
            "FOLD 1 Jaccard 0.6855969923921674\n",
            "\n",
            "#########################\n",
            "### FOLD 2\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.3653 - activation_loss: 1.0831 - activation_1_loss: 1.2822\n",
            "Epoch 00001: val_loss improved from inf to 1.78218, saving model to v0-roberta-1.h5\n",
            "688/688 [==============================] - 295s 429ms/step - loss: 2.3653 - activation_loss: 1.0831 - activation_1_loss: 1.2822 - val_loss: 1.7822 - val_activation_loss: 0.8485 - val_activation_1_loss: 0.9337\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.8341 - activation_loss: 0.8710 - activation_1_loss: 0.9631\n",
            "Epoch 00002: val_loss improved from 1.78218 to 1.70279, saving model to v0-roberta-1.h5\n",
            "688/688 [==============================] - 293s 425ms/step - loss: 1.8341 - activation_loss: 0.8710 - activation_1_loss: 0.9631 - val_loss: 1.7028 - val_activation_loss: 0.8300 - val_activation_1_loss: 0.8728\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6376 - activation_loss: 0.7977 - activation_1_loss: 0.8400\n",
            "Epoch 00003: val_loss improved from 1.70279 to 1.67826, saving model to v0-roberta-1.h5\n",
            "688/688 [==============================] - 292s 425ms/step - loss: 1.6376 - activation_loss: 0.7977 - activation_1_loss: 0.8400 - val_loss: 1.6783 - val_activation_loss: 0.8212 - val_activation_1_loss: 0.8570\n",
            "#####\n",
            "Predicting Validation\n",
            "#####\n",
            "172/172 [==============================] - 25s 144ms/step\n",
            "#####\n",
            "Predicting Test\n",
            "#####\n",
            "111/111 [==============================] - 16s 143ms/step\n",
            "FOLD 2 Jaccard 0.6906972244821378\n",
            "\n",
            "#########################\n",
            "### FOLD 3\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.2261 - activation_loss: 1.0611 - activation_1_loss: 1.1651\n",
            "Epoch 00001: val_loss improved from inf to 1.70524, saving model to v0-roberta-2.h5\n",
            "688/688 [==============================] - 296s 430ms/step - loss: 2.2261 - activation_loss: 1.0611 - activation_1_loss: 1.1651 - val_loss: 1.7052 - val_activation_loss: 0.8685 - val_activation_1_loss: 0.8367\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6794 - activation_loss: 0.8431 - activation_1_loss: 0.8363\n",
            "Epoch 00002: val_loss improved from 1.70524 to 1.62728, saving model to v0-roberta-2.h5\n",
            "688/688 [==============================] - 292s 425ms/step - loss: 1.6794 - activation_loss: 0.8431 - activation_1_loss: 0.8363 - val_loss: 1.6273 - val_activation_loss: 0.8229 - val_activation_1_loss: 0.8043\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.5295 - activation_loss: 0.7736 - activation_1_loss: 0.7559\n",
            "Epoch 00003: val_loss did not improve from 1.62728\n",
            "688/688 [==============================] - 291s 423ms/step - loss: 1.5295 - activation_loss: 0.7736 - activation_1_loss: 0.7559 - val_loss: 1.6976 - val_activation_loss: 0.8542 - val_activation_1_loss: 0.8434\n",
            "#####\n",
            "Predicting Validation\n",
            "#####\n",
            "172/172 [==============================] - 25s 144ms/step\n",
            "#####\n",
            "Predicting Test\n",
            "#####\n",
            "111/111 [==============================] - 16s 143ms/step\n",
            "FOLD 3 Jaccard 0.6961756159437377\n",
            "\n",
            "#########################\n",
            "### FOLD 4\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.2440 - activation_loss: 1.0966 - activation_1_loss: 1.1474\n",
            "Epoch 00001: val_loss improved from inf to 1.67201, saving model to v0-roberta-3.h5\n",
            "688/688 [==============================] - 296s 430ms/step - loss: 2.2440 - activation_loss: 1.0966 - activation_1_loss: 1.1474 - val_loss: 1.6720 - val_activation_loss: 0.8589 - val_activation_1_loss: 0.8131\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.7602 - activation_loss: 0.8673 - activation_1_loss: 0.8929\n",
            "Epoch 00002: val_loss did not improve from 1.67201\n",
            "688/688 [==============================] - 291s 423ms/step - loss: 1.7602 - activation_loss: 0.8673 - activation_1_loss: 0.8929 - val_loss: 1.6780 - val_activation_loss: 0.8666 - val_activation_1_loss: 0.8114\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.5804 - activation_loss: 0.7925 - activation_1_loss: 0.7880\n",
            "Epoch 00003: val_loss improved from 1.67201 to 1.65794, saving model to v0-roberta-3.h5\n",
            "688/688 [==============================] - 293s 426ms/step - loss: 1.5804 - activation_loss: 0.7925 - activation_1_loss: 0.7880 - val_loss: 1.6579 - val_activation_loss: 0.8400 - val_activation_1_loss: 0.8179\n",
            "#####\n",
            "Predicting Validation\n",
            "#####\n",
            "172/172 [==============================] - 25s 144ms/step\n",
            "#####\n",
            "Predicting Test\n",
            "#####\n",
            "111/111 [==============================] - 16s 144ms/step\n",
            "FOLD 4 Jaccard 0.6904126036150283\n",
            "\n",
            "#########################\n",
            "### FOLD 5\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 2.3849 - activation_loss: 1.0955 - activation_1_loss: 1.2894\n",
            "Epoch 00001: val_loss improved from inf to 1.75699, saving model to v0-roberta-4.h5\n",
            "688/688 [==============================] - 296s 430ms/step - loss: 2.3849 - activation_loss: 1.0955 - activation_1_loss: 1.2894 - val_loss: 1.7570 - val_activation_loss: 0.8695 - val_activation_1_loss: 0.8875\n",
            "Epoch 2/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6938 - activation_loss: 0.8462 - activation_1_loss: 0.8475\n",
            "Epoch 00002: val_loss improved from 1.75699 to 1.66032, saving model to v0-roberta-4.h5\n",
            "688/688 [==============================] - 293s 425ms/step - loss: 1.6938 - activation_loss: 0.8462 - activation_1_loss: 0.8475 - val_loss: 1.6603 - val_activation_loss: 0.8549 - val_activation_1_loss: 0.8055\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - ETA: 0s - loss: 1.6245 - activation_loss: 0.8133 - activation_1_loss: 0.8113\n",
            "Epoch 00003: val_loss did not improve from 1.66032\n",
            "688/688 [==============================] - 291s 422ms/step - loss: 1.6245 - activation_loss: 0.8133 - activation_1_loss: 0.8113 - val_loss: 1.7156 - val_activation_loss: 0.8697 - val_activation_1_loss: 0.8459\n",
            "#####\n",
            "Predicting Validation\n",
            "#####\n",
            "172/172 [==============================] - 25s 144ms/step\n",
            "#####\n",
            "Predicting Test\n",
            "#####\n",
            "111/111 [==============================] - 16s 145ms/step\n",
            "FOLD 5 Jaccard 0.6901100121668927\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq_sXN8QEGhT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa66f714-f75d-492b-fb6c-93cfd462a2c6"
      },
      "source": [
        "print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>> OVERALL 5Fold CV Jaccard = 0.6905984897199928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzAIVP0xCuNl",
        "colab_type": "text"
      },
      "source": [
        "###### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnXD2kA6C2rB",
        "colab_type": "text"
      },
      "source": [
        "###### Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRbhrQWaC9Lo",
        "colab_type": "text"
      },
      "source": [
        "###### 2nd phase training"
      ]
    }
  ]
}